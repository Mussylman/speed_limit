# main.py
# Pipeline: YOLO —Ç—Ä–µ–∫–∏–Ω–≥ + —Å–∫–æ—Ä–æ—Å—Ç—å + OCR –Ω–æ–º–µ—Ä–æ–≤ (NomeroffNet)
# OCR –∫–∞–∂–¥—ã–π –∫–∞–¥—Ä, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ª—É—á—à–∏–π –ø–æ confidence

import os
import sys
import cv2
import yaml
import time
import argparse
import datetime
import numpy as np
from typing import Dict, Tuple
from threading import Thread
from queue import Queue
from ultralytics import YOLO


class AsyncVideoWriter:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –≤–∏–¥–µ–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    def __init__(self, path: str, fourcc: int, fps: float, size: Tuple[int, int]):
        self.writer = cv2.VideoWriter(path, fourcc, fps, size)
        self.queue: Queue = Queue(maxsize=30)  # –±—É—Ñ–µ—Ä –Ω–∞ 30 –∫–∞–¥—Ä–æ–≤
        self.running = True
        self.thread = Thread(target=self._write_loop, daemon=True)
        self.thread.start()

    def _write_loop(self):
        while self.running or not self.queue.empty():
            try:
                frame = self.queue.get(timeout=0.1)
                self.writer.write(frame)
            except:
                pass

    def write(self, frame: np.ndarray):
        if not self.queue.full():
            self.queue.put(frame.copy())  # copy –ø–æ—Ç–æ–º—É —á—Ç–æ frame –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è

    def release(self):
        self.running = False
        self.thread.join(timeout=5.0)
        self.writer.release()

# Headless —Ä–µ–∂–∏–º (Docker –±–µ–∑ –¥–∏—Å–ø–ª–µ—è)
HEADLESS = os.environ.get("HEADLESS", "0") == "1" or os.environ.get("DISPLAY_OFF", "0") == "1"

# –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR, "src"))
sys.path.insert(0, os.path.join(BASE_DIR, "nomeroff-net"))

from speed_line import SpeedEstimator
from speed_homography import HomographySpeedEstimator
from plate_recognizer import PlateRecognizer
from video.source import VideoSource, SourceType, create_source_from_config
from speed_logger import SpeedLogger
from metrics_logger import MetricsLogger
from async_ocr import AsyncOCR
from async_yolo import AsyncYOLO
from file_logger import FileLogger

import torch


def print_gpu_info():
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üñ•Ô∏è  GPU: {gpu_name} ({gpu_mem:.1f} GB)")
    else:
        print("‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")


def load_yaml(path):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def draw_results_panel(frame, passed_results, speed_logger, speed_limit):
    """–†–∏—Å—É–µ—Ç –ø–∞–Ω–µ–ª—å —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç—è–º–∏ —Å–ª–µ–≤–∞"""
    h, w = frame.shape[:2]
    panel_width = 280

    # –ë—ã—Å—Ç—Ä—ã–π —Ç—ë–º–Ω—ã–π —Ñ–æ–Ω –±–µ–∑ alpha-blend
    cv2.rectangle(frame, (0, 0), (panel_width, h), (30, 30, 30), -1)

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    cv2.putText(frame, "PASSED VEHICLES", (10, 35),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
    cv2.line(frame, (10, 50), (panel_width - 10, 50), (100, 100, 100), 1)

    # –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    y_offset = 80
    items = list(passed_results.items())[-10:]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10

    for track_id, event in reversed(items):
        plate = event.plate_text

        # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –∏–∑ speed_logger
        speed_event = speed_logger.speeds.get(track_id)
        spd = speed_event.speed_kmh if speed_event else 0

        # –¶–≤–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏
        if spd > speed_limit:
            speed_color = (0, 0, 255)  # –∫—Ä–∞—Å–Ω—ã–π - –Ω–∞—Ä—É—à–µ–Ω–∏–µ
            status = "!"
        else:
            speed_color = (0, 255, 0)  # –∑–µ–ª—ë–Ω—ã–π - –Ω–æ—Ä–º–∞
            status = ""

        # –ù–æ–º–µ—Ä
        cv2.putText(frame, f"{plate}", (15, y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # –°–∫–æ—Ä–æ—Å—Ç—å
        speed_text = f"{spd:.0f} km/h {status}"
        cv2.putText(frame, speed_text, (15, y_offset + 25),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, speed_color, 2)

        y_offset += 60

        if y_offset > h - 50:
            break

    # –°—á—ë—Ç—á–∏–∫ –≤–Ω–∏–∑—É
    total = len(passed_results)
    violations = sum(1 for tid in passed_results
                     if tid in speed_logger.speeds
                     and speed_logger.speeds[tid].speed_kmh > speed_limit)

    cv2.putText(frame, f"Total: {total}  Violations: {violations}", (10, h - 20),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)

    return frame


def load_zones_for_source(source, cfg, cam_cfg, speed):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∑–æ–Ω—ã –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏."""
    line_zones = []

    if source.source_type == SourceType.RTSP and source.info:
        for cam in cam_cfg.get("cameras", []):
            if cam.get("name") == source.info.name:
                line_zones = cam.get("line_zones", [])
                break
    elif "line_zones" in cfg:
        line_zones = cfg["line_zones"]

    if line_zones:
        for zone in line_zones:
            speed.add_line_zone(
                zone["start_line"],
                zone["end_line"],
                zone.get("distance_m", 10),
                direction=zone.get("direction", "down"),
                name=zone.get("name", "Zone"),
                color=tuple(zone.get("color", [0, 255, 0])),
            )
        print(f"üìè –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∑–æ–Ω—ã: {[z.get('name', 'Zone') for z in line_zones]}")

    return line_zones


def process_source(source, cfg, cam_cfg):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    name = source.info.name
    out_dir = os.path.join(BASE_DIR, cfg["output_dir"], f"{name}_run_{timestamp}")
    os.makedirs(out_dir, exist_ok=True)

    print(f"\nüö¶ –°—Ç–∞—Ä—Ç: {name}")
    print(f"üíæ –í—ã—Ö–æ–¥: {out_dir}")
    print(f"üé• Backend: {source.info.backend}")

    # YOLO
    yolo_path = os.path.join(BASE_DIR, cfg["models"]["yolo_model"])
    model = YOLO(yolo_path, task="detect")
    try:
        model.to(cfg["device"])
    except:
        pass

    # FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ GPU –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
    use_half = cfg["device"] == "cuda" and torch.cuda.is_available()
    if use_half:
        print("‚ö° YOLO: FP16 mode enabled")

    # FPS
    fps = source.info.fps if source.info.fps > 0 else cfg.get("fps", 30)

    # –°–∫–æ—Ä–æ—Å—Ç—å
    speed_method = cfg.get("speed_method", "lines")
    show_bird_eye = False

    if speed_method == "homography":
        hom_cfg = cfg.get("homography", {})
        hom_file = os.path.join(BASE_DIR, hom_cfg.get("config_file", "config/homography_config.yaml"))

        if os.path.exists(hom_file):
            speed = HomographySpeedEstimator(
                homography_config=hom_file,
                fps=fps,
                min_track_points=hom_cfg.get("min_track_points", 5),
                smoothing_window=hom_cfg.get("smoothing_window", 10),
                max_speed_kmh=hom_cfg.get("max_speed_kmh", 200),
                min_speed_kmh=hom_cfg.get("min_speed_kmh", 5),
                speed_correction=hom_cfg.get("speed_correction", 1.0),
            )
            show_bird_eye = hom_cfg.get("show_bird_eye", True)
            print(f"üéØ –°–∫–æ—Ä–æ—Å—Ç—å: –ì–û–ú–û–ì–†–ê–§–ò–Ø")
        else:
            speed_method = "lines"

    if speed_method == "lines":
        speed = SpeedEstimator(fps)
        load_zones_for_source(source, cfg, cam_cfg, speed)
        print(f"üìè –°–∫–æ—Ä–æ—Å—Ç—å: –õ–ò–ù–ò–ò")

    # OCR (NomeroffNet) —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
    ocr_conf_threshold = float(cfg.get("ocr_conf_threshold", 0.5))
    plate_format = cfg.get("plate_format_regex", "")
    recognizer = PlateRecognizer(
        output_dir=out_dir,
        camera_id=name,
        min_conf=ocr_conf_threshold,
        min_plate_chars=int(cfg.get("min_plate_chars", 8)),
        min_car_height=int(cfg.get("min_car_height", 150)),
        min_car_width=int(cfg.get("min_car_width", 100)),
        min_plate_width=int(cfg.get("min_plate_width", 60)),
        min_plate_height=int(cfg.get("min_plate_height", 15)),
        cooldown_frames=int(cfg.get("ocr_cooldown_frames", 3)),
        plate_format_regex=plate_format,
    )

    # –õ–æ–≥–≥–µ—Ä —Å–∫–æ—Ä–æ—Å—Ç–∏
    speed_limit = int(cfg.get("speed_limit", 70))
    speed_logger = SpeedLogger(
        output_dir=out_dir,
        camera_id=name,
        speed_limit=speed_limit,
    )

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π OCR —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    async_ocr = AsyncOCR(
        recognizer,
        max_queue_size=64,           # [1] –æ—á–µ—Ä–µ–¥—å —Å –ª–∏–º–∏—Ç–æ–º
        num_workers=1,
        max_crop_width=320,          # [4] resize –ø–µ—Ä–µ–¥ –æ—á–µ—Ä–µ–¥—å—é
        good_conf_threshold=0.88,    # [3] early stop –µ—Å–ª–∏ —Ö–æ—Ä–æ—à–∏–π
        cache_max_size=200,          # [5] –ª–∏–º–∏—Ç –∫—ç—à–∞
        cache_ttl_frames=300,        # [5] TTL ~10 —Å–µ–∫ –ø—Ä–∏ 30fps
    )

    print(f"üî§ OCR: NomeroffNet (ASYNC)")
    print(f"   –ú–∞—à–∏–Ω–∞: >= {cfg.get('min_car_height', 150)}x{cfg.get('min_car_width', 100)} px")
    print(f"   –ù–æ–º–µ—Ä:  >= {cfg.get('min_plate_width', 60)}x{cfg.get('min_plate_height', 15)} px")
    print(f"   –§–æ—Ä–º–∞—Ç: {plate_format if plate_format else '–ª—é–±–æ–π'}")
    print(f"   Cooldown: {cfg.get('ocr_cooldown_frames', 3)} –∫–∞–¥—Ä–æ–≤")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    yolo_imgsz = int(cfg.get("yolo_imgsz", 1280))
    frame_skip = int(cfg.get("frame_skip", 1))
    show_window = bool(cfg.get("show_window", True)) and not HEADLESS

    print(f"üîç YOLO: imgsz={yolo_imgsz}, skip={frame_skip}")

    # –ú–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ live-line, –≤—Å—ë –≤ —Ñ–∞–π–ª—ã)
    metrics = MetricsLogger(
        output_dir=out_dir,
        report_interval=10.0,  # –æ—Ç—á—ë—Ç –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫
        json_log=True,
    )
    metrics.async_ocr = async_ocr

    # –§–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä (–¥–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏)
    file_logger = FileLogger(output_dir=out_dir)
    recognizer.file_logger = file_logger  # –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è OCR

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π YOLO
    async_yolo = AsyncYOLO(
        model=model,
        imgsz=yolo_imgsz,
        classes=[2],
        half=use_half,
        tracker="bytetrack.yaml",
    )
    print(f"‚ö° YOLO: async mode (frame_skip={frame_skip})")

    print("üöó –ó–∞–ø—É—Å–∫... (Q ‚Äî –≤—ã—Ö–æ–¥)\n")

    # –ó–∞–ø–∏—Å—å –≤–∏–¥–µ–æ
    records_dir = os.path.join(BASE_DIR, "records")
    os.makedirs(records_dir, exist_ok=True)
    video_out_path = os.path.join(records_dir, f"{name}_{timestamp}.mp4")
    video_writer = None

    frame_idx = 0

    # –ö—ç—à –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ
    cached_detections = []  # [{box, obj_id, plate_text, plate_conf, speed}, ...]
    last_yolo_frame = 0
    last_yolo_time_ms = 0

    # –ö–æ–Ω—Ç—Ä–æ–ª—å FPS –ø–æ–∫–∞–∑–∞
    target_frame_time = 1.0 / fps
    last_display_time = time.time()

    while True:
        frame_start_time = time.time()

        ok, frame = source.read()

        if not ok or frame is None:
            if show_window:
                cv2.imshow(name, np.zeros((360, 640, 3), dtype=np.uint8))
                if cv2.waitKey(1) & 0xFF == ord("q"):
                    break
            continue

        frame_idx += 1

        # === –û–¢–ü–†–ê–í–ö–ê –ù–ê YOLO (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä) ===
        if frame_skip <= 1 or frame_idx % frame_skip == 0:
            async_yolo.submit(frame, frame_idx)

        # === –ü–û–õ–£–ß–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í YOLO (–Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π) ===
        for yolo_result in async_yolo.get_results():
            metrics.start_frame(yolo_result.frame_idx)
            last_yolo_time_ms = yolo_result.processing_time_ms
            last_yolo_frame = yolo_result.frame_idx

            # –û—á–∏—â–∞–µ–º –∫—ç—à –¥–µ—Ç–µ–∫—Ü–∏–π
            cached_detections = []

            # –î–µ—Ç–µ–∫—Ü–∏–∏ —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –∏ —Å crops
            detections = yolo_result.detections
            confs_list = [d.conf for d in detections]
            metrics.log_yolo(yolo_result.processing_time_ms, len(detections), confs_list)

            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤ —Ñ–∞–π–ª
            file_logger.log_detections(
                frame_idx=yolo_result.frame_idx,
                detections=[{"obj_id": d.obj_id, "box": d.box, "conf": d.conf} for d in detections],
                yolo_time_ms=yolo_result.processing_time_ms,
            )

            for det in detections:
                obj_id = det.obj_id
                x1i, y1i, x2i, y2i = det.box

                # –°–∫–æ—Ä–æ—Å—Ç—å
                speed.process(yolo_result.frame_idx, obj_id, det.cx, det.cy)

                # OCR (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π) - crop —É–∂–µ –≤—ã—Ä–µ–∑–∞–Ω –≤ YOLO thread
                async_ocr.submit(
                    track_id=obj_id,
                    crop=det.crop,
                    car_height=y2i - y1i,
                    car_width=x2i - x1i,
                    frame_idx=yolo_result.frame_idx,
                    detection_conf=det.conf,
                )

                # –ü–æ–ª—É—á–∞–µ–º OCR –∏–∑ –∫—ç—à–∞
                cached_result = async_ocr.get_cached_result(obj_id)
                plate_text = cached_result.plate_text if cached_result else ""
                plate_conf = cached_result.ocr_conf if cached_result else 0.0

                # –°–∫–æ—Ä–æ—Å—Ç—å
                current_speed = None
                if speed_method == "homography":
                    v = speed.get_speed(obj_id)
                    if v:
                        current_speed = v
                else:
                    if hasattr(speed, 'object_speeds') and obj_id in speed.object_speeds:
                        v, _ = speed.object_speeds[obj_id]
                        current_speed = v

                # –õ–æ–≥–∏—Ä—É–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
                if current_speed:
                    is_violation = current_speed > speed_limit
                    speed_logger.update(
                        track_id=obj_id,
                        speed_kmh=current_speed,
                        frame_idx=yolo_result.frame_idx,
                        plate_text=plate_text,
                        plate_conf=plate_conf,
                    )
                    metrics.log_speed(current_speed, is_violation)
                    # –í —Ñ–∞–π–ª
                    file_logger.log_speed(
                        frame_idx=yolo_result.frame_idx,
                        track_id=obj_id,
                        speed_kmh=current_speed,
                        plate_text=plate_text,
                    )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                cached_detections.append({
                    'box': det.box,
                    'obj_id': obj_id,
                    'plate_text': plate_text,
                    'plate_conf': plate_conf,
                    'speed': current_speed,
                })

            # Cleanup
            if speed_method == "homography":
                speed.cleanup_old_tracks(yolo_result.frame_idx)

            # Performance log
            file_logger.log_performance(
                frame_idx=yolo_result.frame_idx,
                fps=metrics.current_fps,
                yolo_ms=yolo_result.processing_time_ms,
                ocr_queue=async_ocr.queue_size(),
                yolo_queue=async_yolo.input_queue.qsize(),
                cars_count=len(detections),
            )

            metrics.update_skip_reasons(recognizer.stats)
            metrics.end_frame()

        # === –ü–û–õ–£–ß–ï–ù–ò–ï OCR –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
        for ocr_result in async_ocr.get_results(current_frame_idx=frame_idx):
            if ocr_result.blur_score > 0:
                metrics.log_quality(ocr_result.blur_score, ocr_result.brightness_score)
            if ocr_result.result and ocr_result.result.plate_text:
                metrics.log_ocr(
                    called=True,
                    time_ms=ocr_result.processing_time_ms,
                    result=ocr_result.result.plate_text,
                    conf=ocr_result.result.ocr_conf,
                    passed=True,
                )

        # === –û–¢–†–ò–°–û–í–ö–ê (–Ω–∞ –ö–ê–ñ–î–û–ú –∫–∞–¥—Ä–µ) ===
        raw_frame = frame.copy()

        # –ó–æ–Ω—ã
        if speed_method == "lines" and hasattr(speed, 'draw_zones'):
            speed.draw_zones(raw_frame)

        # –†–∏—Å—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–∑ –∫—ç—à–∞
        for det in cached_detections:
            x1i, y1i, x2i, y2i = det['box']
            obj_id = det['obj_id']
            plate_text = det['plate_text']
            plate_conf = det['plate_conf']
            current_speed = det['speed']

            box_color = (0, 255, 0) if plate_conf >= ocr_conf_threshold else (0, 255, 255)
            cv2.rectangle(raw_frame, (x1i, y1i), (x2i, y2i), box_color, 2)
            cv2.putText(raw_frame, f"ID:{obj_id}", (x1i, y1i - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, box_color, 2)

            if plate_text:
                cv2.putText(raw_frame, f"{plate_text} ({plate_conf:.2f})",
                            (x1i, y2i + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            if current_speed:
                c = (0, 255, 0) if current_speed < speed_limit else (0, 0, 255)
                cv2.putText(raw_frame, f"{current_speed:.0f} km/h", (x1i, y1i - 35),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, c, 2)

        if show_window:
            # –ü–∞–Ω–µ–ª—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            if recognizer.passed_results:
                raw_frame = draw_results_panel(
                    raw_frame, recognizer.passed_results,
                    speed_logger, speed_limit
                )

            # Bird eye view
            if show_bird_eye and speed_method == "homography":
                bev = speed.draw_bird_eye_view(size=(300, 400), frame_idx=frame_idx)
                bev_h, bev_w = bev.shape[:2]
                raw_frame[10:10+bev_h, raw_frame.shape[1]-bev_w-10:raw_frame.shape[1]-10] = bev

            cv2.putText(raw_frame, f"FPS: {metrics.current_fps:.1f}", (290, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

            # –ó–∞–ø–∏—Å—å –≤–∏–¥–µ–æ
            if video_writer is None:
                h, w = raw_frame.shape[:2]
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                video_writer = AsyncVideoWriter(video_out_path, fourcc, fps, (w, h))
                print(f"üé¨ –ó–∞–ø–∏—Å—å: {video_out_path} (async, {fps:.0f} fps)")
            video_writer.write(raw_frame)

            cv2.imshow(name, raw_frame)

            # === –ö–û–ù–¢–†–û–õ–¨ FPS –ü–û–ö–ê–ó–ê ===
            elapsed = time.time() - frame_start_time
            wait_ms = max(1, int((target_frame_time - elapsed) * 1000))
            if cv2.waitKey(wait_ms) & 0xFF == ord("q"):
                break

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    source.release()
    cv2.destroyAllWindows()

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
    if video_writer is not None:
        video_writer.release()
        print(f"üé¨ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {video_out_path}")

    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
    async_yolo.stop()
    async_ocr.stop()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n")  # –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ live-line
    yolo_stats = async_yolo.get_stats()
    print(f"üìä YOLO: submitted={yolo_stats['submitted']}, processed={yolo_stats['processed']}, dropped={yolo_stats['dropped']}")
    async_ocr.print_stats()
    recognizer.finalize()
    speed_logger.finalize()
    metrics.finalize()

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ: {name}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Speed + ANPR (NomeroffNet)")
    parser.add_argument("--source", required=True, choices=["rtsp", "video", "folder", "image"])
    parser.add_argument("--camera", type=str)
    parser.add_argument("--path", type=str)
    args = parser.parse_args()

    cfg = load_yaml(os.path.join(BASE_DIR, "config", "config.yaml"))
    cam_cfg = load_yaml(os.path.join(BASE_DIR, "config", "config_cam.yaml"))

    print_gpu_info()

    if args.source == "rtsp":
        if not args.camera:
            sys.exit("‚ùó –£–∫–∞–∂–∏—Ç–µ --camera")
        source = create_source_from_config(cam_cfg, args.camera, prefer_hw=True)
    else:
        if not args.path:
            sys.exit("‚ùó –£–∫–∞–∂–∏—Ç–µ --path")
        source_type = {
            "video": SourceType.VIDEO,
            "folder": SourceType.FOLDER,
            "image": SourceType.IMAGE,
        }[args.source]
        source = VideoSource(args.path, source_type=source_type, prefer_hw=True)

    process_source(source, cfg, cam_cfg)
